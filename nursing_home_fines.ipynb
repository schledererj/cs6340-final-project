{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Nursing Home Fines\n",
    "\n",
    "[Direct Supply's](https://www.directsupply.com/) business is focused on senior living and healthcare. We have services in procurement, building management, design and construction, and health and wellness. The ability to predict fines a nursing home might receive would have several benefits to both the nursing home and to Direct Supply. Direct Supply may be able to help the nursing home avoid fines through the products and services that we offer. We can focus sales and engagement efforts around the things we know they need. From Direct Supply's perspective, we may be able to anticipate a downturn in sales, better estimate credit worthiness, or get ahead of potential merger and acquisition activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "\n",
    "* Identify categorical and numeric features\n",
    "* Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.width = 1000\n",
    "\n",
    "# NOTE: This may not be necessary in future versions of scikit-learn, but it is\n",
    "#       necessary here to get the feature names out of the ColumnTransformer.\n",
    "#       All of the other transformers support this method.\n",
    "SimpleImputer.get_feature_names_out = (\n",
    "    lambda self, names=None: self.feature_names_in_\n",
    ")\n",
    "plt.style.use('seaborn-dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with importing the master _Provider Information_ dataset from the US government's CMS website. This inlcudes a wealth of columns that we'll encode to build our base model. It also includes the column \"Total Amount of Fines in Dollars\", which is what our prediction target will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_info_df = pd.read_csv('NH_ProviderInfo_Jan2022.csv')\n",
    "provider_info_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the mathematical summary of the prediction target, or the total fine amount per facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_info_df['Total Amount of Fines in Dollars'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of train-test splitting, we'll want to discretize the total fine amount into a small amount of buckets such that we can use that value to pass to the _stratify_ parameter on scikit-learn's `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(provider_info_df['Total Amount of Fines in Dollars'], bins=10)\n",
    "plt.title('Total Amount of Fines in Dollars', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we may want to remove outliers > $500,000 or so, later, when we improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has many interesting columns. From inspecting them, we've identified the following numerical and categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'Number of Certified Beds', 'Average Number of Residents per Day', 'Overall Rating', 'Health Inspection Rating', 'QM Rating', 'Long-Stay QM Rating', \n",
    "    'Short-Stay QM Rating', 'Staffing Rating', 'RN Staffing Rating', 'Reported Nurse Aide Staffing Hours per Resident per Day',\n",
    "    'Reported LPN Staffing Hours per Resident per Day', 'Reported RN Staffing Hours per Resident per Day', 'Reported Licensed Staffing Hours per Resident per Day',\n",
    "    'Reported Total Nurse Staffing Hours per Resident per Day', 'Total number of nurse staff hours per resident per day on the weekend',\n",
    "    'Registered Nurse hours per resident per day on the weekend', 'Reported Physical Therapist Staffing Hours per Resident Per Day',\n",
    "    'Total nursing staff turnover', 'Registered Nurse turnover',  'Number of administrators who have left the nursing home',\n",
    "    'Case-Mix Nurse Aide Staffing Hours per Resident per Day', 'Case-Mix LPN Staffing Hours per Resident per Day', 'Case-Mix RN Staffing Hours per Resident per Day',\n",
    "    'Case-Mix Total Nurse Staffing Hours per Resident per Day', 'Adjusted Nurse Aide Staffing Hours per Resident per Day',\n",
    "    'Adjusted LPN Staffing Hours per Resident per Day', 'Adjusted RN Staffing Hours per Resident per Day', 'Adjusted Total Nurse Staffing Hours per Resident per Day',\n",
    "    'Rating Cycle 1 Total Number of Health Deficiencies', 'Rating Cycle 1 Number of Standard Health Deficiencies',\n",
    "    'Rating Cycle 1 Number of Complaint Health Deficiencies', 'Rating Cycle 1 Health Deficiency Score', 'Rating Cycle 1 Number of Health Revisits',\n",
    "    'Rating Cycle 1 Health Revisit Score', 'Rating Cycle 1 Total Health Score', 'Rating Cycle 2 Total Number of Health Deficiencies',\n",
    "    'Rating Cycle 2 Number of Standard Health Deficiencies', 'Rating Cycle 2 Number of Complaint Health Deficiencies',\n",
    "    'Rating Cycle 2 Health Deficiency Score', 'Rating Cycle 2 Number of Health Revisits', 'Rating Cycle 2 Health Revisit Score',\n",
    "    'Rating Cycle 2 Total Health Score', 'Rating Cycle 3 Total Number of Health Deficiencies', 'Rating Cycle 3 Number of Standard Health Deficiencies',\n",
    "    'Rating Cycle 3 Number of Complaint Health Deficiencies', 'Rating Cycle 3 Health Deficiency Score', 'Rating Cycle 3 Number of Health Revisits',\n",
    "    'Rating Cycle 3 Health Revisit Score', 'Rating Cycle 3 Total Health Score', 'Total Weighted Health Survey Score', 'Number of Facility Reported Incidents',\n",
    "    'Number of Substantiated Complaints', 'Number of Citations from Infection Control Inspections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'Provider City', 'Provider State', 'Provider Zip Code', 'Provider SSA County Code', 'Ownership Type', 'Provider Type', 'Provider Resides in Hospital', \n",
    "    'Continuing Care Retirement Community', 'Special Focus Status', 'Abuse Icon', 'Most Recent Health Inspection More Than 2 Years Ago', \n",
    "    'Provider Changed Ownership in Last 12 Months', 'With a Resident and Family Council', 'Automatic Sprinkler Systems in All Required Areas',\n",
    "    'Long-Stay QM Rating Footnote', 'Short-Stay QM Rating Footnote', 'Staffing Rating Footnote', 'RN Staffing Rating Footnote', 'Reported Staffing Footnote', \n",
    "    'Physical Therapist Staffing Footnote', 'Total nursing staff turnover footnote', 'Registered Nurse turnover footnote', 'Administrator turnover footnote']\n",
    "for cat_col in cat_cols:\n",
    "    provider_info_df[cat_col] = provider_info_df[cat_col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Baseline Model\n",
    "\n",
    "* Minimal feature engineering\n",
    "* No external datasets\n",
    "* Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms categorical, numerical, and binned features using imputation, scaling, and encoding\n",
    "# Fits the estimator with the training set and returns predicted labels for the X_test set\n",
    "# Train/test splitting and error computation is left to the caller\n",
    "def run_baseline_model(X_train, X_test, y_train, estimator, categorical_features=[], numerical_features=[], binned_features=[], n_bins=15):\n",
    "  numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "  ])\n",
    "\n",
    "  categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "  ])\n",
    "\n",
    "  binning_transformer = Pipeline(steps=[\n",
    "    ('encoder', KBinsDiscretizer(strategy='uniform', n_bins=n_bins))\n",
    "  ])\n",
    "\n",
    "  ct = ColumnTransformer(\n",
    "      remainder='drop',\n",
    "      transformers=[\n",
    "        ('numerical', numeric_transformer, numerical_features),\n",
    "        ('categorical', categorical_transformer, categorical_features),\n",
    "        ('binned', binning_transformer, binned_features)\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  # First we fit the transformers to the training set and transform it\n",
    "  X_train_prepared = pd.DataFrame(\n",
    "    ct.fit_transform(X_train).toarray(),\n",
    "    columns=ct.get_feature_names_out())\n",
    "  \n",
    "  # Then we only transform the test set to make use of the imputation, scaling, \n",
    "  # and categorization parameters from the training set.\n",
    "  X_test_prepared = pd.DataFrame(\n",
    "      ct.transform(X_test).toarray(),\n",
    "      columns=ct.get_feature_names_out())\n",
    "\n",
    "  estimator.fit(X_train_prepared, y_train)\n",
    "  y_pred = estimator.predict(X_test_prepared)\n",
    "\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fines = provider_info_df[['Total Amount of Fines in Dollars']]\n",
    "discretizer = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile')\n",
    "binned_fines = discretizer.fit_transform(fines)\n",
    "sns.histplot(binned_fines)\n",
    "plt.title('Binned Fines', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    provider_info_df, provider_info_df['Total Amount of Fines in Dollars'], \n",
    "    train_size=.75, stratify=binned_fines)\n",
    "\n",
    "y_pred = run_baseline_model(X_train, X_test, y_train, ElasticNet(positive=True), cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: ${rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_test, y_pred, xlabel='Actual Fines (\\$)', ylabel='Predicted Fines (\\$)'):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    sns.scatterplot(x=y_test, y=y_pred)\n",
    "    \n",
    "    all_values = np.concatenate((y_test, y_pred))\n",
    "    x = np.linspace(np.min(all_values), np.max(all_values))\n",
    "    y = x\n",
    "\n",
    "    plt.plot(x, y, color='orange')\n",
    "    plt.title(f\"{xlabel} vs {ylabel}\", fontsize=15)\n",
    "    plt.xlabel(xlabel, fontsize=10)\n",
    "    plt.ylabel(ylabel, fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Feature Engineering\n",
    "\n",
    "Up next, we'll look into our existing column set and see if we can engineer more features from them that may improve the model.\n",
    "\n",
    "### Geocoding\n",
    "\n",
    "In a separate notebook, we used the `geopy`'s geocoding capabilities to reach out to a combination of web services (namely [Nominatim](https://nominatim.org/) and [Bing Maps](https://www.bingmapsportal.com/)) to geocode the addresses of each facility. That is, each address was converted to a latitude and longitude, which were then appended to the dataset as additional columns, and exported to a new CSV file. Let's import that dataset and re-run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_model_df = pd.read_csv('NH_ProviderInfo_Jan2022_with_coords.csv').drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "enhanced_model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add the latitude and longitude as numeric columns, which will be scaled with the `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_num_cols = num_cols + ['lat', 'lon']\n",
    "enhanced_model_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run with the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fines = enhanced_model_df[['Total Amount of Fines in Dollars']]\n",
    "discretizer = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile')\n",
    "binned_fines = discretizer.fit_transform(fines)\n",
    "sns.histplot(binned_fines)\n",
    "plt.title('Binned Fines', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_amount_feature = 'Total Amount of Fines in Dollars'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    enhanced_model_df, enhanced_model_df[fine_amount_feature], \n",
    "    train_size=.75, stratify=binned_fines)\n",
    "\n",
    "y_pred = run_baseline_model(X_train, X_test, y_train, ElasticNet(positive=True), cat_cols, enhanced_num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: ${rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers_df = enhanced_model_df.copy()\n",
    "no_outliers_df = no_outliers_df[enhanced_model_df[fine_amount_feature] < no_outliers_df[fine_amount_feature].quantile(0.99)]\n",
    "no_outliers_df = no_outliers_df[enhanced_model_df[fine_amount_feature] > no_outliers_df[fine_amount_feature].quantile(0.01)]\n",
    "no_outliers_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fines = no_outliers_df[['Total Amount of Fines in Dollars']]\n",
    "discretizer = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile')\n",
    "binned_fines = discretizer.fit_transform(fines)\n",
    "sns.histplot(binned_fines)\n",
    "plt.title('Binned Fines', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    no_outliers_df, no_outliers_df[fine_amount_feature], \n",
    "    train_size=.75, stratify=binned_fines)\n",
    "\n",
    "y_pred = run_baseline_model(X_train, X_test, y_train, ElasticNet(positive=True), cat_cols, enhanced_num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: ${rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is decent improvement so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Datasets\n",
    "\n",
    "Join in state averages and quality MSR data. _Note_ that the MSR set will need to be grouped by the `Federal Provider Number`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_averages_df = pd.read_csv('NH_StateUSAverages_Jan2022.csv')\n",
    "state_averages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_averages_df.columns = state_averages_df.columns.map(lambda x: str(x) + '_State')\n",
    "state_averages_df = state_averages_df.rename(columns={\n",
    "    'State or Nation_State': 'Provider State'\n",
    "}, errors='ignore')\n",
    "state_averages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_df = no_outliers_df.merge(state_averages_df, on='Provider State', how='left')\n",
    "additional_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fields = [x for x in state_averages_df.columns if x not in ['Provider State', 'Processing Date_State', 'Number of Fines_State']]\n",
    "additional_data_num_cols = enhanced_num_cols + new_fields\n",
    "print(len(additional_data_num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    additional_data_df, additional_data_df[fine_amount_feature], \n",
    "    train_size=.75, stratify=binned_fines)\n",
    "\n",
    "y_pred = run_baseline_model(X_train, X_test, y_train, ElasticNet(positive=True), cat_cols, additional_data_num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: ${rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much improvement from bringing in state averages.\n",
    "\n",
    "### Next dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_mds_df = pd.read_csv('NH_QualityMsr_MDS_Jan2022.csv')\n",
    "quality_mds_df['Federal Provider Number'] = quality_mds_df['Federal Provider Number'].map(lambda x: str(x)[1:] if str(x)[0] == '0' else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footnote_cols = [x for x in quality_mds_df.columns if x.startswith('Footnote')]\n",
    "quality_mds_df = quality_mds_df.drop(columns=['Provider Name', 'Provider Address', 'Provider City', 'Provider State', 'Provider Zip Code', 'Location', 'Processing Date', 'Measure Period', 'Measure Description', 'Measure Code'] + footnote_cols, errors='ignore')\n",
    "quality_mds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_mds_agg = quality_mds_df.groupby(['Federal Provider Number']).mean()\n",
    "quality_mds_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_mds_agg.columns = quality_mds_agg.columns.map(lambda x: str(x) + '_mds')\n",
    "quality_mds_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_mds_df = additional_data_df.merge(quality_mds_agg, on='Federal Provider Number', how='left')\n",
    "data_with_mds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_mds_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['Q1 Measure Score_mds', 'Q2 Measure Score_mds', 'Q3 Measure Score_mds', 'Q4 Measure Score_mds', 'Four Quarter Average Score_mds']\n",
    "num_cols_with_mds = additional_data_num_cols + new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_with_mds_df, data_with_mds_df[fine_amount_feature], \n",
    "    train_size=.75, stratify=binned_fines)\n",
    "\n",
    "y_pred = run_baseline_model(X_train, X_test, y_train, ElasticNet(positive=True), cat_cols, num_cols_with_mds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: ${rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Model Types\n",
    "\n",
    "Perhaps we can use some cross validation here to select a model or use grid search to tune model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Model Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Model for Explanatory Purposes\n",
    "\n",
    "Perhaps use a correlation matrix to find features that don't provide much and remove them."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "309b80e4fc7bf4a3e1c535dfc5b8cab1855247bffb23052a40ca007a9a83543c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
